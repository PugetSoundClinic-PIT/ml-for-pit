{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7 -- LLMs and Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* In lecture 6 we learned and applied pre-trained models for sentiment, summarization, named entity recognition, and more.\n",
    "\n",
    "* Each of the models in lecture 6 were unique and separate from each other.\n",
    "\n",
    "* the most recent trend in NLP is to use a single model for many tasks by prompting an incredibly large model (often refered to as \"large-language-models\" or LLMs) with a few words prior to the input text.\n",
    "\n",
    "* this technique can be incredibly powerful, but also requires a lot of data and compute and often, you may need to pay to use an API to access these models rather than directly using them directly in your code\n",
    "\n",
    "* in this lecture we will first learn the basics of interacting with LLM APIs and then we will learn how to prompt them to extract pieces of information from text and then we will learn how to ensure structured output from these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "* you may have used ChatGPT in the past.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting\n",
    "\n",
    "* prompting is a technique that allows us to use a single model for many tasks.\n",
    "\n",
    "* the idea is that we can provide a few words of context to the model and then ask it to complete the rest of the text.\n",
    "\n",
    "* for example, if we want to use a model to summarize text, we can provide the model with a few words of context and then ask it to complete the summary.\n",
    "\n",
    "* prompting is an active area of research and development but this general idea is refered to as \"in-context learning\" i.e. the model learning what to do based on the context provided to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM APIs\n",
    "\n",
    "* due to the fact that there are so many LLMs and so many companies which support using them via API, instead of learning any single API, we will instead demonstrate how to use [LangChain](https://python.langchain.com/docs/get_started/introduction) which has implementations and supporting utilities for working with many of the most popular LLMs (and companies).\n",
    "\n",
    "* for this lecture we will be using the HuggingFace Inference API which allows for 30,000 free response tokens per month. This isn't much to work with but is enough to get a feel for how to use LLMs. In larger projects you will likely need to pay for access to these APIs or pay for total usage.\n",
    "\n",
    "* we are specifically choosing to use the HuggingFace Inference API and LangChain together because both of these tools are fully open source (the source code for langchain and the models are available on huggingface directly), rather than using GPT-4 from OpenAI or another private model.\n",
    "\n",
    "* we first need to create an account on HuggingFace and go through the API key generation process [follow instructions at some link]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prompting w/ Mixtral-8x7B-Instruct-v0.1\n",
    "\n",
    "* we will first use the Mixtral-8x7B-Instruct-v0.1 model for summarization of a portion of a meeting passage.\n",
    "\n",
    "* this involves setting up langchain, connecting to the HuggingFace API, and then prompting the model and providing some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup langchain and ask for a summary for a part of a meeting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* using this same passage of text, we can prompt for other things however, such as simple pieces of information like \"which city council is this a meeting of?\" or \"what is the date of this meeting?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt for the council name and the meeting date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems\n",
    "\n",
    "* however, note that each time we run this snippet, we may get a slightly different answer or maybe a different structure.\n",
    "\n",
    "* there are two common methods to solve this problem:\n",
    "\n",
    "1. we can turn down the tempurature of the model to make it more likely to produce the same output each time. \"tempurature\" is a parameter we can provide which determines how \"creative\" the model is allowed to be. A tempurature of 0.0 means that the model will always produce the same output, while a tempurature of 1.0 means that the model will be allowed to be as creative as it wants. A tempurature of 0.5 is a good starting point for most tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the same two prompts again, but this time turn the tempurature down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. we can ask for structured output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-for-pit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
