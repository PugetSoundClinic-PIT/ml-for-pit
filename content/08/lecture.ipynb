{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 8 -- Analysis and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "* We have learned how many different tools work, how to apply them, and how to train and build some of our own models.\n",
    "\n",
    "* however, we have not yet taken the step of applying these tools together in a meaningful way.\n",
    "\n",
    "* in this chapter we are going to do just that\n",
    "\n",
    "* we will use the tools we have learned to answer a question by analyzing a dataset and visualizing and reporting our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming Up With Questions\n",
    "\n",
    "* there are a lot of questions we can ask about this data. but generally a good place to start is by asking something simple that may reveal a larger pattern for further investigation\n",
    "\n",
    "* for example, lets try to answer the question \"how are public comment's typically structured in city council meetings?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planning\n",
    "\n",
    "* our first step in this process is to break down how we can answer our question\n",
    "\n",
    "* within each meeting, we will want to identify the sentences / portions which are public comments -- when you are working on identification problems, **classification** is always a good place to start\n",
    "\n",
    "* we will also want to look into the structure. a few examples of public comments might be (note, the names and organizations are made up, but the sentence structure comes directly from meetings of the Seattle City Council):\n",
    "\n",
    "  * Good afternoon, Councilmembers. My name is Lisa and I'm speaking for Parents for Police today. And we, I'd like to support, here to support the appointment of Bill as Executive Director of the CPC. ...\n",
    "  * Hello, I'm Bryce Chin, Chair of Justice Washington and a District 7 constituent. I am speaking in opposition to the SPOG MOU. ...\n",
    "  * Hi, council members. My name is Tim Brown and I live on Capitol Hill and run a small business in Soto. I'm calling today to urge you to vote no on the SPOG memo of understanding. ...\n",
    "\n",
    "* we can see that there are a few different structures here. some people introduce themselves, some people introduce their organization, some people introduce their district, they usually say what they are speaking about, and then they say what they want the council to do\n",
    "\n",
    "* To start, lets try to breakdown comments by:\n",
    "  * \"self-introduction\" - the speaker introduces themselves\n",
    "  * \"org-introduction\" - the speaker can optionally introduce themselves but additionally introduces an organization\n",
    "  * \"other\" - the speaker does not introduce themselves or an organization\n",
    "\n",
    "* this is a complex task, and one we can likely train yet another classification model for but we might want to use something to help us annotate data\n",
    "\n",
    "* before we do any of this, we will probably want to work with a small dataset before scaling up to the entire corpus\n",
    "\n",
    "* lets start by trying to identify the start and end of the public comment / public hearing sections of the meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdp_data import CDPInstances, datasets\n",
    "\n",
    "sessions = datasets.get_session_dataset(\n",
    "    CDPInstances.Seattle,\n",
    "    start_datetime=\"2021-01-01\",\n",
    "    end_datetime=\"2022-01-01\",\n",
    "    store_transcript=True,\n",
    "    raise_on_error=False,\n",
    ")\n",
    "sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Dataset for Training a Public Comment Period Start and End Classifier\n",
    "\n",
    "* the first thing we will want to do is to identify the start and end of the public comment sections\n",
    "\n",
    "* to do so, lets construct a dataset for annotation\n",
    "\n",
    "* we are going to try and help ourselves as much as possible with the annotation by using a few tools\n",
    "\n",
    "* first, we will iterate over a sample of the meetings and read the transcript\n",
    "\n",
    "* we will then embed each of the sentences within each transcript and calculate the cosine similarity between the current sentence and \"ideal\" sentences for the start and end of a public comment section.\n",
    "\n",
    "* these usually look like \"the public comment period is now open\" and \"the public comment period is now closed\"\n",
    "\n",
    "* for each meeting, we we will then take the top three most similar sentences for of the \"ideal\" sentences and build a dataset for annotation\n",
    "\n",
    "* further, we will use something call \"negative sampling\" to help gather up examples that are by estimate, not likely to be related to the start or end of a public comment section\n",
    "    * more reading on \"negative sampling\" can be found here: http://deepdive.stanford.edu/generating_negative_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdp_backend.pipeline.transcript_model import Transcript\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Init the model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed the \"ideal\" open and close statements\n",
    "pc_open = \"The public comment period is now open\"\n",
    "pc_closed = \"The public comment period is now closed\"\n",
    "pc_open_embed = model.encode(pc_open)\n",
    "pc_closed_embed = model.encode(pc_closed)\n",
    "\n",
    "# Sampled sessions\n",
    "samples_sessions = sessions.sample(100)\n",
    "\n",
    "# For each session, open the transcript,\n",
    "# get the top three most similar sentences to the open and close statements\n",
    "n_random_sentences = 10\n",
    "statements_for_annotation = []\n",
    "for _, session in tqdm(\n",
    "    samples_sessions.iterrows(),\n",
    "    total=len(samples_sessions),\n",
    "    desc=\"Sessions\",\n",
    "):\n",
    "    # Read transcript\n",
    "    with open(session.transcript_path) as open_f:\n",
    "        transcript = Transcript.from_json(open_f.read())\n",
    "\n",
    "    # Get the top three most similar sentences to the open statement\n",
    "    open_similarities = cos_sim(\n",
    "        pc_open_embed,\n",
    "        model.encode([s.text for s in transcript.sentences]),\n",
    "    ).squeeze()\n",
    "    open_similarities = pd.Series(open_similarities)\n",
    "    open_similarities = open_similarities.sort_values(ascending=False)\n",
    "    open_similarities = open_similarities[:3]\n",
    "\n",
    "    # Get sentences associated with top three\n",
    "    open_sentences = [\n",
    "        transcript.sentences[i] for i in open_similarities.index\n",
    "    ]\n",
    "    # Add these to the dataframe\n",
    "    open_sentences_df = pd.DataFrame(\n",
    "        {\n",
    "            \"infra\": [CDPInstances.Seattle] * len(open_sentences),\n",
    "            \"session_id\": [session[\"id\"]] * len(open_sentences),\n",
    "            \"sentence\": [s.text for s in open_sentences],\n",
    "            \"label\": [\"\"] * len(open_sentences),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Get the top three most similar sentences to the close statement\n",
    "    close_similarities = cos_sim(\n",
    "        pc_closed_embed,\n",
    "        model.encode([s.text for s in transcript.sentences]),\n",
    "    ).squeeze()\n",
    "    close_similarities = pd.Series(close_similarities)\n",
    "    close_similarities = close_similarities.sort_values(ascending=False)\n",
    "    close_similarities = close_similarities[:3]\n",
    "    \n",
    "    # Get sentences associated with top three\n",
    "    close_sentences = [\n",
    "        transcript.sentences[i] for i in close_similarities.index\n",
    "    ]\n",
    "    # Add these to the dataframe\n",
    "    close_sentences_df = pd.DataFrame(\n",
    "        {\n",
    "            \"infra\": [CDPInstances.Seattle] * len(close_sentences),\n",
    "            \"session_id\": [session[\"id\"]] * len(close_sentences),\n",
    "            \"sentence\": [s.text for s in close_sentences],\n",
    "            \"label\": [\"\"] * len(close_sentences),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Also add n_random_sentences random sentences from the transcript pre-labeled as False\n",
    "    # This is called negative sampling\n",
    "    # The main idea is that because the transcripts have so much more data than just the open and close statements\n",
    "    # we can randomly select some sentences and assume they are not open or close statements\n",
    "    # This is a good way to get a lot of negative examples\n",
    "    random_negative_samples = np.random.choice(\n",
    "        transcript.sentences, size=n_random_sentences, replace=False\n",
    "    )\n",
    "    random_negative_samples_df = pd.DataFrame(\n",
    "        {\n",
    "            \"infra\": [CDPInstances.Seattle] * len(random_negative_samples),\n",
    "            \"session_id\": [session[\"id\"]] * len(random_negative_samples),\n",
    "            \"sentence\": [s.text for s in random_negative_samples],\n",
    "            \"label\": [\"other\"] * len(random_negative_samples),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Concatenate the open and close dataframes\n",
    "    statements_for_annotation.append(\n",
    "        pd.concat([open_sentences_df, close_sentences_df, random_negative_samples_df])\n",
    "    )\n",
    "\n",
    "# Concatenate all the dataframes in the statements_for_annotation list\n",
    "statements_for_annotation = pd.concat(statements_for_annotation)\n",
    "\n",
    "# It's likely that some of the sentences are duplicates\n",
    "# We can remove these by dropping duplicates\n",
    "statements_for_annotation = statements_for_annotation.drop_duplicates(\n",
    "    subset=[\"session_id\", \"sentence\"],\n",
    ")\n",
    "\n",
    "statements_for_annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We now have a nice dataframe ready for export and annotation\n",
    "* For each sentence, we can label it as either:\n",
    "    * \"start\" - the start of a public comment section\n",
    "    * \"end\" - the end of a public comment section\n",
    "    * \"other\" - neither the start nor the end of a public comment section\n",
    "\n",
    "* we can then export this dataframe to a csv file for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV\n",
    "statements_for_annotation.to_csv(\n",
    "    \"public_comment_start_end_other_statements_for_annotation.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* once the data has been annotated, we can then load it back into python and train a classifier\n",
    "\n",
    "* for a quick overview of the dataset, we can print the value counts of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_statements = pd.read_csv(\n",
    "    \"annotated_public_comment_start_end_other_statements.csv\"\n",
    ")\n",
    "annotated_statements.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lots and lots of examples of other\n",
    "* almost double the examples of start vs end\n",
    "* this is somewhat expected as there is a lot of administrative cruft that occurs before the start of a public comment section\n",
    "* and during annotation, we annotated any statement that seemed to be a part of that cruft as \"start\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Public Comment Period Start and End Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import precision_recall_fscore_support, ConfusionMatrixDisplay\n",
    "\n",
    "# Function for quickly training model and evaluating on test set\n",
    "def train_and_eval_model(\n",
    "    data: pd.DataFrame,\n",
    ") -> tuple[LogisticRegressionCV, float, float, float, ConfusionMatrixDisplay, pd.DataFrame]:\n",
    "    # Create train and test splits\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data[\"sentence\"],\n",
    "        data[\"label\"],\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=data[\"label\"],\n",
    "    )\n",
    "\n",
    "    # Create embedded text values\n",
    "    embedded_x_train = model.encode(x_train.values, show_progress_bar=True)\n",
    "    embedded_x_test = model.encode(x_test.values, show_progress_bar=True)\n",
    "\n",
    "    # Train model\n",
    "    clf = LogisticRegressionCV(random_state=42, max_iter=500)\n",
    "    clf.fit(embedded_x_train, y_train)\n",
    "\n",
    "    # Eval\n",
    "    y_pred = clf.predict(embedded_x_test)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "    print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "\n",
    "    # Confusion per label\n",
    "    matrix = ConfusionMatrixDisplay.from_predictions(y_test, y_pred, labels=clf.classes_)\n",
    "\n",
    "    # Create dataframe of misclassified examples\n",
    "    misclassified = pd.DataFrame(\n",
    "        {\n",
    "            \"sentence\": x_test,\n",
    "            \"label\": y_test,\n",
    "            \"prediction\": y_pred,\n",
    "        }\n",
    "    )\n",
    "    misclassified = misclassified[misclassified[\"label\"] != misclassified[\"prediction\"]]\n",
    "\n",
    "    return clf, precision, recall, f1, matrix, misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and eval model\n",
    "clf, precision, recall, f1, matrix, misclassified = train_and_eval_model(\n",
    "    annotated_statements\n",
    ")\n",
    "\n",
    "# Print stats\n",
    "print(f\"Precision: {precision:.3f}, Recall: {recall:.3f}, F1: {f1:.3f}\")\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These seem okay! Only two sentences are misclassified! And in both cases they seem odd.\n",
    "    * in one it seems like there may have been a typo or the speaker misspoke because they say \"public comment period is now open\" even though there are no speakers.\n",
    "    * in the other, the speaker says \"Public comment period is open and closed\" which happens sometimes when no one signs up to speak.\n",
    "\n",
    "* Granted, we didn't use too much data and we have yet to test it on meetings outside of Seattle but this is a good start for us to use further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our model to extract public comment periods from meetings\n",
    "\n",
    "* Now lets try and build a function to use this model to find public comment periods within a meeting\n",
    "\n",
    "* We will first classify each sentence in meeting using the classifer\n",
    "\n",
    "* Once we have found a sentence that is classified as a public comment period \"start\", we will both, continue to look for further \"start\" sentences (as there may be later administrative cruft), additionally we will then look for the next sentence that is classified as a the \"end\" of a public comment period\n",
    "\n",
    "* we will then return the sentences between the last start and the first end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_public_comment_periods(\n",
    "    transcript_path: str,\n",
    ") -> list[str]:\n",
    "    # Read in the transcript\n",
    "    with open(transcript_path) as open_f:\n",
    "        transcript = Transcript.from_json(open_f.read())\n",
    "    \n",
    "    # Init the list of public comment periods\n",
    "    pc_periods = []\n",
    "    last_open_index = -1\n",
    "\n",
    "    # Iter over sentences and classify each one with the open classifier\n",
    "    # Once we have found a sentence that is classified as a public comment open\n",
    "    # we will both, continue to look for future classifications of public comment open\n",
    "    # (as there may be administrative cruft and we want the _last_ public comment open)\n",
    "    # and we will start looking for the close statement\n",
    "    # additionally, we will look for the close statement\n",
    "    # once we find one, we will extract all of the sentences between the last open and the first close\n",
    "    # as a single public comment period\n",
    "\n",
    "    # note, there may be multiple public comment periods in a single meeting\n",
    "    # so we will return a list of public comment periods\n",
    "    for i, sentence in enumerate(transcript.sentences):\n",
    "        # Embed the sentence\n",
    "        embedded_sentence = model.encode(sentence.text)\n",
    "\n",
    "        # Shape the embedded sentence for sklearn\n",
    "        embedded_sentence = embedded_sentence[np.newaxis,:]\n",
    "\n",
    "        # Predict if the sentence is a public comment open\n",
    "        pred = clf.predict(embedded_sentence)[0]\n",
    "\n",
    "        # If it is a public comment open\n",
    "        if pred == \"start\":\n",
    "            # Update the last open index\n",
    "            last_open_index = i\n",
    "        \n",
    "        # If we have seen a public comment open\n",
    "        elif last_open_index > -1:\n",
    "            # If it is a public comment close\n",
    "            if pred == \"end\":\n",
    "                # Get all the sentences between the last open and the close\n",
    "                pc_period = transcript.sentences[last_open_index:i+1]\n",
    "                # Add the pc period to the list\n",
    "                pc_periods.append([s.text for s in pc_period])\n",
    "                # Reset the last open index\n",
    "                last_open_index = -1\n",
    "\n",
    "    return pc_periods\n",
    "\n",
    "# Try our function on a single random transcript\n",
    "np.random.seed(8)\n",
    "example_session = sessions.sample(1).iloc[0]\n",
    "get_public_comment_periods(example_session.transcript_path)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is great! It seems like we have a function which can extract public comment periods from meetings\n",
    "\n",
    "* Normally you would want to evaluate this function / method as well by annotating meetings but for now we will use it as is\n",
    "\n",
    "* Let's extract the public comment periods from each meeting and save them to a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the public comment periods for all the transcripts\n",
    "all_pc_periods = []\n",
    "for _, session in tqdm(\n",
    "    sessions.iterrows(),\n",
    "    total=len(sessions),\n",
    "    desc=\"Sessions\",\n",
    "):\n",
    "    this_session_pc_periods = get_public_comment_periods(session.transcript_path)\n",
    "    \n",
    "    # Add a row to the dataframe for each public comment period\n",
    "    for pc_period in this_session_pc_periods:\n",
    "        all_pc_periods.append(\n",
    "            {\n",
    "                \"infra\": CDPInstances.Seattle,\n",
    "                \"session_id\": session[\"id\"],\n",
    "                \"pc_period\": pc_period,\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert to dataframe\n",
    "all_pc_periods = pd.DataFrame(all_pc_periods)\n",
    "all_pc_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It seems like out of the original 226 sessions we gathered, we have 135 sessions with public comment periods\n",
    "\n",
    "* lets also calculate the mean number of public comment periods per meeting and find any meetings with multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean number of public comment periods per session\n",
    "all_pc_periods.groupby(\"session_id\").pc_period.count().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataframe of pc_periods for sessions that have multiple pc_periods\n",
    "sessions_with_multiple_pc_periods = all_pc_periods.groupby(\"session_id\").pc_period.count()\n",
    "sessions_with_multiple_pc_periods = sessions_with_multiple_pc_periods[sessions_with_multiple_pc_periods > 1]\n",
    "len(sessions_with_multiple_pc_periods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* so it seems like there are only 10 sessions in our dataset with multiple public comment periods\n",
    "* this can happen but in Seattle is somewhat rare so this somewhat makes sense but again, we would want to evaluate this function further\n",
    "* the last thing we will do is to save the dataframe to a parquet file for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe for next analysis\n",
    "all_pc_periods.to_parquet(\"all_pc_periods.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Individual Public Comments from Public Comment Periods\n",
    "\n",
    "* using the same approach as before we will now try to extract individual public comments from each public comment period\n",
    "\n",
    "* this time however, we will make a classifier of:\n",
    "  * \"public-comment-clerk-transition\": the transitions between the clerk and each commenter (i.e. \"thank you our next speaker is...\")\n",
    "  * \"other\": any other sentence\n",
    "\n",
    "* to do so, we will simply take a random sample of these pc_periods and annotate each sentence as either \"public-comment-clerk-transition\" or \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe\n",
    "all_pc_periods = pd.read_parquet(\"all_pc_periods.parquet\")\n",
    "all_pc_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* take a random sample of the data, then use all of the sentences in the period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sampled periods\n",
    "sampled_pc_periods = all_pc_periods.sample(15)\n",
    "\n",
    "# Iter pc_periods\n",
    "statements_for_annotation = []\n",
    "for _, row in tqdm(\n",
    "    sampled_pc_periods.iterrows(),\n",
    "    total=len(sampled_pc_periods),\n",
    "    desc=\"Public Comment Periods\",\n",
    "):\n",
    "    # For each sentence within the pc_period, create a row in the dataframe\n",
    "    for sentence in row.pc_period:\n",
    "        statements_for_annotation.append(\n",
    "            {\n",
    "                \"infra\": row.infra,\n",
    "                \"session_id\": row.session_id,\n",
    "                \"sentence\": sentence,\n",
    "                \"label\": \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "# Convert to dataframe\n",
    "statements_for_annotation = pd.DataFrame(statements_for_annotation)\n",
    "statements_for_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statements_for_annotation.to_csv(\n",
    "    \"individual_public_comment_sentences_for_annotation.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* after annotation we can train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>infra</th>\n",
       "      <th>session_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>b3c9f239a5c3</td>\n",
       "      <td>But for now you can make general comments in t...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>b3c9f239a5c3</td>\n",
       "      <td>Go ahead, you have one minute.</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>b3c9f239a5c3</td>\n",
       "      <td>Thank you.</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>b3c9f239a5c3</td>\n",
       "      <td>Dear city council members, I'm a resident of d...</td>\n",
       "      <td>comment-introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>b3c9f239a5c3</td>\n",
       "      <td>As you all sit here today you have the opportu...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>f45c27e8169e</td>\n",
       "      <td>Please consider the cost of these capital upgr...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>f45c27e8169e</td>\n",
       "      <td>Thank you.</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>f45c27e8169e</td>\n",
       "      <td>Thank you very much.</td>\n",
       "      <td>comment-end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>f45c27e8169e</td>\n",
       "      <td>I'm BJ.</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>cdp-seattle-21723dcf</td>\n",
       "      <td>f45c27e8169e</td>\n",
       "      <td>BJ will be followed by Janine Falls.</td>\n",
       "      <td>comment-end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1200 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     infra    session_id  \\\n",
       "0     cdp-seattle-21723dcf  b3c9f239a5c3   \n",
       "1     cdp-seattle-21723dcf  b3c9f239a5c3   \n",
       "2     cdp-seattle-21723dcf  b3c9f239a5c3   \n",
       "3     cdp-seattle-21723dcf  b3c9f239a5c3   \n",
       "4     cdp-seattle-21723dcf  b3c9f239a5c3   \n",
       "...                    ...           ...   \n",
       "1195  cdp-seattle-21723dcf  f45c27e8169e   \n",
       "1196  cdp-seattle-21723dcf  f45c27e8169e   \n",
       "1197  cdp-seattle-21723dcf  f45c27e8169e   \n",
       "1198  cdp-seattle-21723dcf  f45c27e8169e   \n",
       "1199  cdp-seattle-21723dcf  f45c27e8169e   \n",
       "\n",
       "                                               sentence                 label  \n",
       "0     But for now you can make general comments in t...                 other  \n",
       "1                        Go ahead, you have one minute.                 other  \n",
       "2                                            Thank you.                 other  \n",
       "3     Dear city council members, I'm a resident of d...  comment-introduction  \n",
       "4     As you all sit here today you have the opportu...                 other  \n",
       "...                                                 ...                   ...  \n",
       "1195  Please consider the cost of these capital upgr...                 other  \n",
       "1196                                         Thank you.                 other  \n",
       "1197                               Thank you very much.           comment-end  \n",
       "1198                                            I'm BJ.                 other  \n",
       "1199               BJ will be followed by Janine Falls.           comment-end  \n",
       "\n",
       "[1200 rows x 4 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "annotated_individual_pc_sentences = pd.read_csv(\"annotated_individual_public_comment_sentences.csv\")\n",
    "\n",
    "# Drop any row without a label\n",
    "annotated_individual_pc_sentences = annotated_individual_pc_sentences.dropna(subset=[\"label\"])\n",
    "annotated_individual_pc_sentences.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Init the model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "individual_sentence_clf, isentence_pre, isentence_rec, isentence_f1, matrix, misclassified = (\n",
    "    train_and_eval_model(annotated_individual_pc_sentences)\n",
    ")\n",
    "print(f\"Precision: {isentence_pre:.3f}, Recall: {isentence_rec:.3f}, F1: {isentence_f1:.3f}\")\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get individual public comments within a public comment period\n",
    "def get_segmented_public_comments(\n",
    "    pc_period: list[str],\n",
    ") -> list[list[str]]: \n",
    "    # Init the list of individual public comments\n",
    "    public_comments = []\n",
    "    \n",
    "    # Iter over sentences and classify each one\n",
    "    # Once we have found a sentence that is classified as a clerk transition\n",
    "    # we will both, continue to look for future classifications of clerk transition\n",
    "    # (as there may be administrative cruft and we want the _last_ clerk transition sentence)\n",
    "    # once we have moved off of the clerk transition, we will start looking for the next clerk transition\n",
    "    # once we find one, we will extract all of the sentences between the last clerk transition and the\n",
    "    # current one\n",
    "    current_clerk_transition_index = -1\n",
    "    for i, sentence in enumerate(pc_period):\n",
    "        # Embed the sentence\n",
    "        embedded_sentence = model.encode(sentence)\n",
    "\n",
    "        # Shape the embedded sentence for sklearn\n",
    "        embedded_sentence = embedded_sentence[np.newaxis,:]\n",
    "\n",
    "        # Predict if the sentence is a clerk transition\n",
    "        pred = individual_sentence_clf.predict(embedded_sentence)[0]\n",
    "        print(pred)\n",
    "        print(sentence)\n",
    "        print(current_clerk_transition_index)\n",
    "        print()\n",
    "\n",
    "        # If it is a clerk transition\n",
    "        if pred == \"clerk-or-chair-transition\":\n",
    "            # Update the current clerk transition index\n",
    "            current_clerk_transition_index = i\n",
    "        \n",
    "        # If we have seen a clerk transition\n",
    "        elif current_clerk_transition_index != -1:\n",
    "            # If it is a public comment\n",
    "            if pred == \"other\":\n",
    "                # Get all the sentences between the last clerk transition and the public comment\n",
    "                public_comment = pc_period[current_clerk_transition_index:i+1]\n",
    "                # Add the public comment to the list\n",
    "                public_comments.append(public_comment)\n",
    "                # Reset the current clerk transition index\n",
    "                current_clerk_transition_index = -1\n",
    "        \n",
    "    return public_comments\n",
    "\n",
    "# Try our function on a single random transcript\n",
    "np.random.seed(1)\n",
    "comments = get_segmented_public_comments(all_pc_periods.sample(1).iloc[0].pc_period)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-for-pit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
